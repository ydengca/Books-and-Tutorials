{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/alvinjin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data from Keras\n",
    "Images are ecoded as Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: (60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Dataset Shape: {}\".format(train_images.shape))\n",
    "train_images[0]\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 4 samples in Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1188839e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_images[0], cmap='gray')\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_images[1], cmap='gray')\n",
    "plt.subplot(223)\n",
    "plt.imshow(test_images[2], cmap='gray')\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_images[3], cmap='gray')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalize/Scale data\n",
    "For a multi-layer feedforward model we must reduce the images down into a vector of pixels. \n",
    "Reshape data by flattening 28x28 images to a 784 vector for each image.\n",
    "\n",
    "The pixel values are gray scale between 0 and 255. Normalize it 0-1 range by dividing 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0], 28 * 28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "val_images = train_images[:5000]\n",
    "train_images = train_images[5000:]\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0], 28*28))\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot-encoding on Labels\n",
    "It is good practice to use a one-hot-encoding of the class values, transforming the vector of class integers into a binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firt 10 Training Labels: [7 2 1 0 4 1 4 9 5 9]\n",
      "After encoding: [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print(\"Firt 10 Training Labels: {}\".format(test_labels[:10]))\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "# Split Validataion set from training set\n",
    "val_labels = train_labels[:5000]\n",
    "train_labels = train_labels[5000:]\n",
    "\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"After encoding: {}\".format(test_labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "nn = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer consists 784 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.add(layers.Dense(units=784, activation='sigmoid', input_shape=(28*28,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784)\n"
     ]
    }
   ],
   "source": [
    "print(nn.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer consists of 10 neurons since we want to classify digits into 10 classes (0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.add(layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "print(nn.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "nn.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy']\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.4428 - acc: 0.8806 - val_loss: 0.2761 - val_acc: 0.9182\n",
      "Epoch 2/10\n",
      " - 9s - loss: 0.2569 - acc: 0.9265 - val_loss: 0.2128 - val_acc: 0.9408\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.2040 - acc: 0.9400 - val_loss: 0.1727 - val_acc: 0.9518\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.1642 - acc: 0.9525 - val_loss: 0.1457 - val_acc: 0.9582\n",
      "Epoch 5/10\n",
      " - 10s - loss: 0.1342 - acc: 0.9610 - val_loss: 0.1237 - val_acc: 0.9648\n",
      "Epoch 6/10\n",
      " - 9s - loss: 0.1097 - acc: 0.9685 - val_loss: 0.1145 - val_acc: 0.9680\n",
      "Epoch 7/10\n",
      " - 10s - loss: 0.0906 - acc: 0.9739 - val_loss: 0.1015 - val_acc: 0.9706\n",
      "Epoch 8/10\n",
      " - 9s - loss: 0.0766 - acc: 0.9772 - val_loss: 0.0937 - val_acc: 0.9726\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.0648 - acc: 0.9812 - val_loss: 0.0824 - val_acc: 0.9734\n",
      "Epoch 10/10\n",
      " - 9s - loss: 0.0541 - acc: 0.9844 - val_loss: 0.0795 - val_acc: 0.9752\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(train_images, train_labels,\n",
    "      epochs=10,\n",
    "      batch_size=128,\n",
    "      validation_data=(val_images, val_labels),\n",
    "      verbose=2\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 623,290\n",
      "Trainable params: 623,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 164us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.081302840868569912, 0.97519999999999996]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes: [7 2 1 0]\n",
      "Predicted Probabilities:\n",
      " [[  1.11434497e-06   9.33532007e-09   1.34320144e-05   3.10329488e-04\n",
      "    1.46044776e-08   4.46380363e-06   8.91787963e-11   9.99621630e-01\n",
      "    2.27702076e-05   2.61553632e-05]\n",
      " [  3.64010225e-06   9.23414191e-04   9.98629332e-01   2.96562619e-04\n",
      "    1.44427512e-10   1.00012359e-04   2.42189471e-05   2.67816064e-11\n",
      "    2.28822209e-05   2.01905478e-10]\n",
      " [  5.72864110e-07   9.94812608e-01   2.47634191e-04   1.49351719e-04\n",
      "    4.25747749e-05   1.08485256e-04   7.76028246e-05   2.54830043e-03\n",
      "    1.93681801e-03   7.60285111e-05]\n",
      " [  9.99456704e-01   1.74804393e-06   5.39883113e-05   1.83309749e-05\n",
      "    9.57466818e-06   8.33767408e-05   2.15334439e-04   7.30045358e-05\n",
      "    3.78326126e-06   8.42789013e-05]]\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict_classes(test_images[:4])\n",
    "probs = nn.predict_proba(test_images[:4])\n",
    "\n",
    "print(\"Predicted Classes: {}\".format(preds))\n",
    "print(\"Predicted Probabilities:\\n {}\".format(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
